# LLM + RAG ‚Äì Code de l‚Äô√©ducation (Projet exp√©rimental)

---

## üá´üá∑ Partie 1 ‚Äî Description en fran√ßais

### Objectif du projet
Ce projet explore la conception d‚Äôun **assistant IA local** combinant :
- un **LLM local** (via Ollama),
- un **pipeline RAG (Retrieval-Augmented Generation)**,
- appliqu√© au **Code de l‚Äô√©ducation fran√ßais**.

L‚Äôobjectif principal est de **comprendre, tester et structurer** une approche robuste permettant :
- d‚Äôinterroger un corpus juridique complexe,
- de limiter les hallucinations des mod√®les de langage,
- de produire des r√©ponses tra√ßables et v√©rifiables,
- tout en restant dans un environnement **local et ma√Ætris√©**.

Ce projet s‚Äôinscrit dans une d√©marche **exp√©rimentale et p√©dagogique**.

---

###  √âtat actuel du projet

#### Environnement technique
- Syst√®me : Ubuntu (WSL)
- GPU : NVIDIA RTX 4060 (8 Go)
- Environnement Python isol√© : `llm_code_education_env`
- Gestion du LLM local via **Ollama**

####  Donn√©es
- Source actuelle : **Code de l‚Äô√©ducation au format PDF**
- Extraction du texte page par page
- Nettoyage l√©ger du texte
- D√©coupage en chunks
- Vectorisation avec `sentence-transformers`
- Stockage dans un **index FAISS local**

#### Pipeline RAG (texte)
- Question utilisateur en entr√©e
- Recherche s√©mantique dans l‚Äôindex FAISS
- Injection du contexte pertinent dans le LLM local
- G√©n√©ration de r√©ponses textuelles bas√©es sur le contexte r√©cup√©r√©

#### Voix (exp√©rimentation en cours)
- Int√©gration technique d‚Äôun moteur de **speech-to-text local**
- Validation du flux audio ‚Üí texte
- R√©flexion en cours sur une int√©gration plus ergonomique via navigateur

---

### Enseignements cl√©s √† ce stade
- Les performances d‚Äôun RAG d√©pendent fortement de la **qualit√© et de la structure de la source**.
- Un corpus juridique en PDF impose des contraintes importantes :
  - structure implicite,
  - r√©f√©rences juridiques indirectes,
  - pagination non normative.
- Les LLM n√©cessitent des **garde-fous explicites** pour √©viter des r√©ponses plausibles mais incorrectes.
- La s√©paration claire entre :
  - r√©cup√©ration de l‚Äôinformation,
  - g√©n√©ration de la r√©ponse,
  est essentielle pour am√©liorer la fiabilit√©.

---

### Prochaines √©tapes
- Mise en place d‚Äôun **backend FastAPI** unifi√©
- Capture audio c√¥t√© navigateur (Web Audio API)
- Pipeline STT int√©gr√© au backend
- Renforcement du RAG avec :
  - validation explicite des sources,
  - citations construites c√¥t√© code,
- √âtude d‚Äôune source juridique plus structur√©e (XML / L√©gifrance)

---

### Avertissement
Ce projet est **exp√©rimental**.  
Les r√©ponses produites :
- ne constituent pas un avis juridique,
- peuvent √™tre incompl√®tes ou inexactes,
- doivent toujours √™tre v√©rifi√©es √† partir des sources officielles.

---

## üá¨üáß Part 2 ‚Äî English description

### Project goal
This project explores the design of a **local AI assistant** combining:
- a **local LLM** (via Ollama),
- a **RAG (Retrieval-Augmented Generation) pipeline**,
- applied to the **French Code of Education**.

The main objective is to **understand, test, and structure** a robust approach to:
- query a complex legal corpus,
- reduce LLM hallucinations,
- produce traceable and verifiable answers,
- while keeping everything **local and controlled**.

This is an **experimental and educational** project.

---

### Current project status

#### Technical environment
- System: Ubuntu (WSL)
- GPU: NVIDIA RTX 4060 (8 GB)
- Isolated Python environment: `llm_code_education_env`
- Local LLM management via **Ollama**

#### Data
- Current source: **French Code of Education (PDF format)**
- Page-by-page text extraction
- Light text cleaning
- Chunking
- Embeddings with `sentence-transformers`
- Local **FAISS vector index**

#### RAG pipeline (text-based)
- User question as input
- Semantic search in FAISS
- Injection of relevant context into the local LLM
- Text-based answer generation grounded in retrieved context

#### Voice (ongoing experimentation)
- Technical validation of a **local speech-to-text** engine
- Audio ‚Üí text pipeline validated
- Ongoing reflection on browser-based integration for better UX

---

### Key insights so far
- RAG performance strongly depends on **data structure and quality**.
- PDF-based legal corpora introduce significant constraints:
  - implicit structure,
  - indirect legal references,
  - non-normative pagination.
- LLMs require **explicit safeguards** to avoid plausible but incorrect answers.
- A clear separation between:
  - information retrieval,
  - answer generation,
  is critical to improve reliability.

---

### Next steps
- Unified **FastAPI backend**
- Browser-side audio capture (Web Audio API)
- Backend-integrated STT pipeline
- Stronger RAG with:
  - explicit source validation,
  - code-enforced citations,
- Evaluation of more structured legal sources (XML / L√©gifrance)

---

### Disclaimer
This project is **experimental**.  
The generated answers:
- do not constitute legal advice,
- may be incomplete or inaccurate,
- must always be verified against official sources.
